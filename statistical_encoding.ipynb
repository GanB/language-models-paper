{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM2IkMa0UBdTxxOXzOwzeaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GanB/language-models-paper/blob/master/statistical_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "jYumwnxhAfik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is a process used in natural language processing and information retrieval to reduce words to their base or root form, known as the \"stem.\" The purpose of stemming is to normalize words so that variations of the same word can be treated as identical during analysis or search. For example, the words \"run,\" \"running,\" and \"ran\" would all be reduced to the stem \"run.\"\n",
        "\n",
        "Stemming algorithms typically remove common suffixes from words to extract the stem. This process involves rules or heuristics that operate on the word's structure to remove endings such as \"-s,\" \"-ed,\" or \"-ing.\" The resulting stem may not always be a valid word or may be a partial word, but it serves as a common representation for related word forms.\n",
        "\n",
        "Stemming can be useful in various natural language processing tasks, such as text mining, information retrieval, and search engines. By reducing words to their stems, it becomes easier to group similar words together and perform operations like searching or indexing based on the root form of a word. However, it's important to note that stemming algorithms are not always perfect and may produce incorrect stems or remove parts that change the word's meaning.\n",
        "\n",
        "Popular stemming algorithms:\n",
        "\n",
        "* Porter Stemming Algorithm:  It applies a set of rules to strip common English suffixes from words. It is simple and efficient but may sometimes produce stems that are not actual words.\n",
        "\n",
        "* Snowball Stemming Algorithm:  Snowball is an extension of the Porter algorithm. It supports multiple languages and provides more accurate stemming than the original Porter algorithm. Snowball allows for easier customization and the addition of new languages.\n",
        "\n",
        "* Lancaster Stemming Algorithm: The Lancaster stemming algorithm is an aggressive stemming algorithm that applies a series of rules to remove suffixes from words. It is known for its fast execution speed but can sometimes produce very aggressive stems, leading to more drastic reductions than other algorithms.\n",
        "\n",
        "* Lovins Stemming Algorithm: The Lovins stemming algorithm is based on a set of stemming rules developed by J. C. Lovins. It focuses on reducing words to their root forms by applying various transformations. This algorithm is often used for information retrieval tasks.\n",
        "\n",
        "* Porter2 Stemming Algorithm (also known as the English Stemmer): This is an updated version of the Porter stemming algorithm, designed to improve the accuracy of stemming for English words. It addresses some of the limitations of the original Porter algorithm and is commonly used in search engines and information retrieval systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "4exj1PhhCpjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['studying', 'studied', 'running', 'ran', 'sleeping', 'slept', 'flies',\n",
        "         'dies', 'meeting', 'talking', 'talks', 'talked', 'cherries',\n",
        "         'generously', 'cats', 'better', 'rocks', 'wolves' ]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"PorterStemmer:\")\n",
        "\n",
        "for word, stem in zip(words, stemmed_words):\n",
        "    print(f\"Word: {word} | Stem: {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYABIzOsAn52",
        "outputId": "56dd9b4c-b2e1-411c-9d94-9b5e58fb2595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer:\n",
            "Word: studying | Stem: studi\n",
            "Word: studied | Stem: studi\n",
            "Word: running | Stem: run\n",
            "Word: ran | Stem: ran\n",
            "Word: sleeping | Stem: sleep\n",
            "Word: slept | Stem: slept\n",
            "Word: flies | Stem: fli\n",
            "Word: dies | Stem: die\n",
            "Word: meeting | Stem: meet\n",
            "Word: talking | Stem: talk\n",
            "Word: talks | Stem: talk\n",
            "Word: talked | Stem: talk\n",
            "Word: cherries | Stem: cherri\n",
            "Word: generously | Stem: gener\n",
            "Word: cats | Stem: cat\n",
            "Word: better | Stem: better\n",
            "Word: rocks | Stem: rock\n",
            "Word: wolves | Stem: wolv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "words = ['studying', 'studied', 'running', 'ran', 'sleeping', 'slept', 'flies',\n",
        "         'dies', 'meeting', 'talking', 'talks', 'talked', 'cherries',\n",
        "         'generously', 'cats', 'better', 'rocks', 'wolves' ]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"SnowballStemmer:\")\n",
        "for word, stem in zip(words, stemmed_words):\n",
        "    print(f\"Word: {word} | Stem: {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb00CMVcGFNY",
        "outputId": "8d029faa-036f-4137-fcca-3009703d5263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SnowballStemmer:\n",
            "Word: studying | Stem: studi\n",
            "Word: studied | Stem: studi\n",
            "Word: running | Stem: run\n",
            "Word: ran | Stem: ran\n",
            "Word: sleeping | Stem: sleep\n",
            "Word: slept | Stem: slept\n",
            "Word: flies | Stem: fli\n",
            "Word: dies | Stem: die\n",
            "Word: meeting | Stem: meet\n",
            "Word: talking | Stem: talk\n",
            "Word: talks | Stem: talk\n",
            "Word: talked | Stem: talk\n",
            "Word: cherries | Stem: cherri\n",
            "Word: generously | Stem: generous\n",
            "Word: cats | Stem: cat\n",
            "Word: better | Stem: better\n",
            "Word: rocks | Stem: rock\n",
            "Word: wolves | Stem: wolv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "Q0M6jTkQE0MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a natural language processing technique that aims to determine the base or dictionary form of a word, called the \"lemma.\" Unlike stemming, which truncates words to their root form, lemmatization considers the word's context and grammatical role to derive the canonical form.\n",
        "\n",
        "The process of lemmatization involves analyzing words based on their part of speech (POS) tags, such as noun, verb, adjective, adverb, etc., and applying morphological rules to transform them to their base form. This base form is typically a valid word that can be found in a dictionary.\n",
        "\n",
        "For example, the lemma of the word \"running\" would be \"run,\" and the lemma of \"better\" would be \"good.\" Lemmatization takes into account factors like tense, plurality, and inflection to ensure accurate normalization.\n",
        "\n",
        "Lemmatization offers more accurate results compared to stemming because it considers the context and semantics of words. It can be useful in various natural language processing applications, such as text analysis, information retrieval, machine translation, and sentiment analysis.\n",
        "\n",
        "However, lemmatization can be computationally more expensive than stemming due to the need for dictionary lookups and morphological analysis. It also requires the availability of linguistic resources like POS taggers and lemmatization rules specific to the language being processed.\n",
        "\n",
        "Popular Lemmatization algorithms:\n",
        "\n",
        "* WordNet Lemmatizer: WordNet is a lexical database that includes information about word senses and relationships. The WordNet lemmatizer maps words to their corresponding lemmas based on the WordNet database. It is commonly used in applications that require English lemmatization.\n",
        "\n",
        "* Stanford CoreNLP Lemmatizer: Stanford CoreNLP is a popular natural language processing toolkit that provides lemmatization functionality. Its lemmatizer utilizes a combination of rule-based approaches and machine learning techniques to determine the lemma of a word. It supports multiple languages and can handle various word forms and inflections.\n",
        "\n",
        "* spaCy Lemmatizer: spaCy is a widely used library for natural language processing in Python. It includes a lemmatizer component that applies lemmatization based on the word's POS tag and syntactic dependencies. spaCy supports multiple languages and provides accurate lemmatization results.\n",
        "\n",
        "* TreeTagger: TreeTagger is a part-of-speech tagger and lemmatizer developed by the Natural Language Processing Group at the University of Stuttgart. It utilizes a combination of rule-based and stochastic methods to perform lemmatization for several languages. TreeTagger is known for its accuracy and wide language coverage.\n",
        "\n",
        "* Morpha: Morpha is a lemmatization tool that applies morphological analysis to derive lemmas. It uses finite-state transducers to handle different word forms and inflections. Morpha is particularly useful for English lemmatization and is known for its speed and accuracy.\n"
      ],
      "metadata": {
        "id": "5ziPIYsLE3au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeQxVzd3LFr3",
        "outputId": "3c327bae-454a-461b-9924-a6d8f52731b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['studying', 'studied', 'running', 'ran', 'sleeping', 'slept', 'flies',\n",
        "         'dies', 'meeting', 'talking', 'talks', 'talked', 'cherries',\n",
        "         'generously', 'cats', 'better', 'rocks', 'wolves' ]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "\n",
        "print('WordNetLemmatizer:')\n",
        "for word, lemma in zip(words, lemmatized_words):\n",
        "    print(f\"Word: {word} | Lemma: {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSO384J-IrTe",
        "outputId": "22dd3171-829f-414b-d053-38ce5f3b88c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNetLemmatizer:\n",
            "Word: studying | Lemma: studying\n",
            "Word: studied | Lemma: studied\n",
            "Word: running | Lemma: running\n",
            "Word: ran | Lemma: ran\n",
            "Word: sleeping | Lemma: sleeping\n",
            "Word: slept | Lemma: slept\n",
            "Word: flies | Lemma: fly\n",
            "Word: dies | Lemma: dy\n",
            "Word: meeting | Lemma: meeting\n",
            "Word: talking | Lemma: talking\n",
            "Word: talks | Lemma: talk\n",
            "Word: talked | Lemma: talked\n",
            "Word: cherries | Lemma: cherry\n",
            "Word: generously | Lemma: generously\n",
            "Word: cats | Lemma: cat\n",
            "Word: better | Lemma: better\n",
            "Word: rocks | Lemma: rock\n",
            "Word: wolves | Lemma: wolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "lemmatized_words = []\n",
        "for word, pos in pos_tags:\n",
        "    if pos.startswith('N'):  # Noun\n",
        "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
        "    elif pos.startswith('V'):  # Verb\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    elif pos.startswith('J'):  # Adjective\n",
        "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
        "    elif pos.startswith('R'):  # Adverb\n",
        "        lemma = lemmatizer.lemmatize(word, pos='r')\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "    lemmatized_words.append(lemma)\n",
        "\n",
        "for token, pos, lemma in zip(tokens, pos_tags, lemmatized_words):\n",
        "    print(f\"Token: {token} | POS: {pos[1]} | Lemma: {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUnaQpviMdCJ",
        "outputId": "ae2c22db-9777-4b7d-9708-d525385d667f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The | POS: DT | Lemma: The\n",
            "Token: quick | POS: JJ | Lemma: quick\n",
            "Token: brown | POS: NN | Lemma: brown\n",
            "Token: fox | POS: NN | Lemma: fox\n",
            "Token: jumps | POS: VBZ | Lemma: jump\n",
            "Token: over | POS: IN | Lemma: over\n",
            "Token: the | POS: DT | Lemma: the\n",
            "Token: lazy | POS: JJ | Lemma: lazy\n",
            "Token: dog | POS: NN | Lemma: dog\n",
            "Token: . | POS: . | Lemma: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "P0t6CvtaTcKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer is a feature extraction technique commonly used in natural language processing (NLP) to convert a collection of text documents into a matrix of token counts.\n",
        "\n",
        "The CountVectorizer performs the following steps:\n",
        "\n",
        "* Tokenization: It breaks down the text into individual words or terms called tokens. It removes punctuation and converts the text to lowercase by default. It also allows customization of tokenization rules.\n",
        "\n",
        "* Vocabulary Building: It constructs a vocabulary of unique tokens from the text data. Each unique token becomes a feature in the resulting matrix. The vocabulary is typically represented as a dictionary where the keys are the tokens, and the values are the indices or positions of the tokens in the matrix.\n",
        "\n",
        "* Counting: It counts the occurrence of each token in each document. The resulting matrix, called the document-term matrix, represents the frequency of tokens in the text data. Each row corresponds to a document, and each column corresponds to a token in the vocabulary. The matrix contains the count of each token in each document.\n",
        "\n",
        "The resulting document-term matrix can be used as input to various machine learning algorithms, such as classification or clustering algorithms. It represents the text data in a numerical form that algorithms can understand and process."
      ],
      "metadata": {
        "id": "ntmrKWXAUnIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"I love cats\",\n",
        "    \"I love dogs\",\n",
        "    \"Cats and dogs are pets\",\n",
        "    \"Dogs are loyal\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"Document-Term Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "IJZR80-1Tgl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5987bc-70f8-4015-e2bb-c056d0e9f100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'are' 'cats' 'dogs' 'love' 'loyal' 'pets']\n",
            "Document-Term Matrix:\n",
            "[[0 0 1 0 1 0 0]\n",
            " [0 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 1]\n",
            " [0 1 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF\n"
      ],
      "metadata": {
        "id": "AkRm6QqqANtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used in natural language processing and information retrieval to determine the importance of a term (word) in a document relative to a collection of documents.\n",
        "\n",
        "TF-IDF combines two components: term frequency (TF) and inverse document frequency (IDF).\n",
        "\n",
        "* Term Frequency (TF): It measures the frequency of a term (word) within a document. It is calculated by dividing the number of occurrences of a term in a document by the total number of terms in that document. The intuition behind TF is that the more frequent a term is in a document, the more likely it is to be important for that document.\n",
        "\n",
        "* Inverse Document Frequency (IDF): It measures the rarity or uniqueness of a term across all documents in a collection. IDF is calculated by taking the logarithm of the ratio between the total number of documents and the number of documents containing the term. The intuition behind IDF is that terms that occur in a smaller number of documents are more informative and carry more weight compared to terms that occur in a larger number of documents.\n",
        "\n",
        "The TF-IDF score of a term in a document is obtained by multiplying its term frequency (TF) in the document with the inverse document frequency (IDF) of the term.\n",
        "\n",
        "The TF-IDF representation of a document collection can be used for various tasks, such as document retrieval, text classification, and information extraction. It allows us to represent documents in a numerical form that captures the importance of terms within the documents and across the collection."
      ],
      "metadata": {
        "id": "hgzEf1TRQhKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example text documents\n",
        "documents = [\n",
        "    \"I love cats\",\n",
        "    \"I love dogs\",\n",
        "    \"Cats and dogs are pets\",\n",
        "    \"Dogs are loyal\",\n",
        "]\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Learn the vocabulary and transform the documents into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary (tokens) and the TF-IDF matrix\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ygElM3TciTM",
        "outputId": "5af55fbc-ab93-410b-9cd0-82ae016cfb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'are' 'cats' 'dogs' 'love' 'loyal' 'pets']\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.70710678 0.         0.70710678 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.62922751 0.77722116 0.\n",
            "  0.        ]\n",
            " [0.52338122 0.41263976 0.41263976 0.33406745 0.         0.\n",
            "  0.52338122]\n",
            " [0.         0.55349232 0.         0.44809973 0.         0.70203482\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Hot Encoding"
      ],
      "metadata": {
        "id": "YdiwSN0YTUxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a process used to represent categorical variables as binary vectors. It is a technique commonly employed in machine learning and data preprocessing tasks.\n",
        "\n",
        "In one-hot encoding, each category or value in a categorical variable is transformed into a binary vector representation. This binary vector has the length equal to the number of unique categories in the variable. It contains all zeros except for a single one at the index corresponding to the category.\n",
        "\n",
        "Here's an example to illustrate one-hot encoding:\n",
        "\n",
        "Suppose we have a categorical variable \"Color\" with three possible categories: \"Red\", \"Green\", and \"Blue\". One-hot encoding would transform this variable into three binary features: \"Color_Red\", \"Color_Green\", and \"Color_Blue\".\n",
        "\n",
        "Color\tColor_Red\tColor_Green\tColor_Blue\n",
        "Red\t1\t0\t0\n",
        "Green\t0\t1\t0\n",
        "Blue\t0\t0\t1\n",
        "In this example, each row represents an instance or observation with a specific color. The one-hot encoded representation indicates the presence of a particular color by setting the corresponding binary feature to 1 and the others to 0.\n",
        "\n",
        "One-hot encoding is useful for machine learning algorithms as it allows categorical variables to be represented in a format that can be easily understood and processed by models. It enables the inclusion of categorical information in numerical computations and avoids any ordinal relationship assumption between categories."
      ],
      "metadata": {
        "id": "II0tjdNuHk33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzPOEVm-FVtF",
        "outputId": "c3eb287f-99ad-4af8-e327-7d57c13ff944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categories = ['Red', 'Green', 'Blue']\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "encoded_data = encoder.fit_transform([[category] for category in categories]).toarray()\n",
        "\n",
        "print(encoded_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"I love cats\",\n",
        "    \"I love dogs\",\n",
        "    \"Cats and dogs are pets\",\n",
        "    \"Dogs are loyal\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"One-Hot Encoded Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3H-KT2AKBR_",
        "outputId": "eb1c639e-f105-43eb-d1dd-f21efe897bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'are' 'cats' 'dogs' 'love' 'loyal' 'pets']\n",
            "One-Hot Encoded Matrix:\n",
            "[[0 0 1 0 1 0 0]\n",
            " [0 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 1]\n",
            " [0 1 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Gain\n",
        "\n"
      ],
      "metadata": {
        "id": "LNe4HZ5VbNvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain is a concept used in the field of machine learning and decision trees to measure the effectiveness of a particular attribute in classifying or predicting a target variable. It quantifies the amount of information provided by an attribute in reducing the uncertainty about the target variable.\n",
        "\n",
        "In the context of decision trees, information gain is typically used to determine the best attribute to split the data at each node of the tree. The attribute with the highest information gain is chosen as the splitting criterion because it provides the most useful information for making predictions.\n",
        "\n",
        "To calculate information gain, the concept of entropy is used. Entropy is a measure of impurity or disorder in a set of examples. A set with low entropy means that the examples are predominantly of the same class, while a high entropy indicates a more evenly distributed set.\n",
        "\n",
        "The information gain of an attribute is calculated by taking the entropy of the original set and subtracting the weighted average of the entropies of the subsets created by splitting the data on that attribute. The attribute with the highest information gain is chosen as the best split.\n",
        "\n",
        "In summary, information gain quantifies the reduction in entropy achieved by splitting the data on a particular attribute and is used to select the most informative attribute for decision tree construction or feature selection in machine learning tasks."
      ],
      "metadata": {
        "id": "5mn2sDlGbSr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_entropy(data):\n",
        "    # Count the occurrences of each label in the dataset\n",
        "    label_counts = {}\n",
        "    for row in data:\n",
        "        label = row[-1]\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "        label_counts[label] += 1\n",
        "\n",
        "    # Calculate the entropy\n",
        "    entropy = 0.0\n",
        "    num_examples = len(data)\n",
        "    for label in label_counts:\n",
        "        probability = label_counts[label] / num_examples\n",
        "        entropy -= probability * math.log2(probability)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def split_data(data, attribute_index, attribute_value):\n",
        "    # Split the data based on the given attribute and its value\n",
        "    subsets = []\n",
        "    for row in data:\n",
        "        if row[attribute_index] == attribute_value:\n",
        "            subset = row[:attribute_index] + row[attribute_index+1:]\n",
        "            subsets.append(subset)\n",
        "\n",
        "    return subsets\n",
        "\n",
        "def calculate_information_gain(data, attribute_index):\n",
        "    # Calculate the information gain for the given attribute\n",
        "\n",
        "    # Calculate the entropy of the original dataset\n",
        "    entropy_original = calculate_entropy(data)\n",
        "\n",
        "    # Get the unique values of the attribute\n",
        "    attribute_values = set(row[attribute_index] for row in data)\n",
        "\n",
        "    # Calculate the weighted average entropy of the subsets\n",
        "    weighted_entropy = 0.0\n",
        "    num_examples = len(data)\n",
        "    for value in attribute_values:\n",
        "        subsets = split_data(data, attribute_index, value)\n",
        "        subset_entropy = calculate_entropy(subsets)\n",
        "        subset_weight = len(subsets) / num_examples\n",
        "        weighted_entropy += subset_weight * subset_entropy\n",
        "\n",
        "    # Calculate the information gain\n",
        "    information_gain = entropy_original - weighted_entropy\n",
        "\n",
        "    return information_gain\n",
        "\n",
        "# Example usage\n",
        "dataset = [\n",
        "    ['Short', 'No', 'No', 'Not spam'],\n",
        "    ['Long', 'Yes', 'No', 'Spam'],\n",
        "    ['Long', 'No', 'Yes', 'Spam'],\n",
        "    ['Short', 'No', 'No', 'Not spam'],\n",
        "    ['Medium', 'No', 'No', 'Not spam']\n",
        "]\n",
        "\n",
        "# Calculate the information gain for each attribute\n",
        "attribute_indices = [0, 1, 2]\n",
        "for attribute_index in attribute_indices:\n",
        "    information_gain = calculate_information_gain(dataset, attribute_index)\n",
        "    print(f\"Information Gain for Attribute {attribute_index}: {information_gain}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3jOL4GbUtL",
        "outputId": "b4849abc-e425-47a6-9f10-b86d0070062a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gain for Attribute 0: 0.9709505944546686\n",
            "Information Gain for Attribute 1: 0.3219280948873623\n",
            "Information Gain for Attribute 2: 0.3219280948873623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lupyne[graphql,rest]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSETpUNYjXZ8",
        "outputId": "3ca3811e-f89f-4408-d3aa-eaeca5f2f41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lupyne[graphql,rest]\n",
            "  Downloading lupyne-3.0-py3-none-any.whl (27 kB)\n",
            "Collecting fastapi (from lupyne[graphql,rest])\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting strawberry-graphql[asgi]>=0.84.4 (from lupyne[graphql,rest])\n",
            "  Downloading strawberry_graphql-0.192.0-py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest])\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (4.6.3)\n",
            "Collecting python-multipart<0.0.7,>=0.0.5 (from strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest])\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette>=0.18.0 (from strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest])\n",
            "  Downloading starlette-0.28.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi->lupyne[graphql,rest]) (1.10.9)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.18.0->strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (3.7.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.18.0->strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.18.0->strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.18.0->strawberry-graphql[asgi]>=0.84.4->lupyne[graphql,rest]) (1.1.1)\n",
            "Installing collected packages: python-multipart, lupyne, graphql-core, strawberry-graphql, starlette, fastapi\n",
            "Successfully installed fastapi-0.99.1 graphql-core-3.2.3 lupyne-3.0 python-multipart-0.0.6 starlette-0.27.0 strawberry-graphql-0.192.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Dirichlet Allocation (LDA)"
      ],
      "metadata": {
        "id": "x9-wUd1Ree3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Dirichlet Allocation (LDA) is a generative statistical model used for topic modeling. It is a popular algorithm for analyzing collections of documents and identifying the underlying topics that occur within them. LDA assumes that each document is a mixture of various topics, and each topic is characterized by a distribution of words.\n",
        "\n",
        "In LDA, a document is represented as a probability distribution over topics, and each topic is represented as a probability distribution over words. The model assumes that the documents are generated in the following way:\n",
        "\n",
        "The number of words in the document is determined.\n",
        "For each word in the document:\n",
        "a. Randomly choose a topic from the document's topic distribution.\n",
        "b. Randomly choose a word from the chosen topic's word distribution.\n",
        "The process of generating the documents is reversed during the inference phase, where given a collection of documents, LDA aims to discover the underlying topic distributions and word distributions that are most likely to have generated the observed data.\n",
        "\n",
        "LDA uses a Dirichlet prior to model the topic distributions and word distributions. The Dirichlet distribution is a continuous probability distribution over the simplex (a multi-dimensional generalization of a triangle) that is commonly used to model distributions of proportions. It allows the topic and word distributions to have a smooth distribution of probabilities over the possible values.\n",
        "\n",
        "By applying LDA to a collection of documents, it becomes possible to extract the underlying topics and understand the themes that exist within the corpus. LDA has applications in various fields, including information retrieval, natural language processing, text mining, and recommendation systems."
      ],
      "metadata": {
        "id": "c9TMvTyTej2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The sky is blue\",\n",
        "    \"The sun is bright\",\n",
        "    \"The sun in the sky is bright\",\n",
        "    \"We can see the shining sun, the bright sun\"\n",
        "]\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# Create a dictionary from the tokenized documents\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# Convert tokenized documents to bag of words representation\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Perform LDA\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
        "\n",
        "# Print the topics and their corresponding words\n",
        "for topic_id, topic_words in lda_model.show_topics(num_topics=-1, num_words=5):\n",
        "    print(f\"Topic {topic_id}: {topic_words}\")\n",
        "\n",
        "# Get the topic distribution for a specific document\n",
        "document_index = 2\n",
        "document_topics = lda_model.get_document_topics(corpus[document_index])\n",
        "print(f\"Topic distribution for document {document_index}: {document_topics}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As_-_eQ8eoJ9",
        "outputId": "294ac8f5-b5ea-4de1-9cc9-5436e0d516c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.214*\"the\" + 0.167*\"is\" + 0.119*\"sky\" + 0.118*\"bright\" + 0.118*\"sun\"\n",
            "Topic 1: 0.168*\"the\" + 0.101*\"sun\" + 0.101*\"bright\" + 0.099*\"sun,\" + 0.099*\"we\"\n",
            "Topic distribution for document 2: [(0, 0.92483795), (1, 0.075162075)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lucene"
      ],
      "metadata": {
        "id": "ju3wU0Lcfucw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At its core, Lucene is a full-text search library that allows developers to create, index, and search large volumes of text data. It provides an inverted index structure, which enables fast search operations by mapping terms to the documents that contain them. This allows for efficient retrieval of relevant documents based on user queries.\n",
        "\n",
        "Some key features of Lucene include:\n",
        "\n",
        "Indexing: Lucene supports the indexing of documents, which involves tokenizing, analyzing, and storing the textual content of documents. It provides a flexible API for indexing various types of data, such as plain text, HTML, XML, and more.\n",
        "\n",
        "Searching: Lucene allows for both simple and complex search queries. It supports various query types, including term queries, phrase queries, wildcard queries, fuzzy queries, and more. Lucene provides efficient algorithms for scoring and ranking search results based on relevance.\n",
        "\n",
        "Ranking: Lucene employs scoring models to determine the relevance of documents to a given query. It uses factors such as term frequency, inverse document frequency, and vector space models to assign scores to documents and rank them accordingly.\n",
        "\n",
        "Text Analysis: Lucene includes a range of text analysis capabilities, such as tokenization, stemming, stop-word removal, and synonym expansion. These features enhance search accuracy and help handle different linguistic variations.\n",
        "\n",
        "Highlighting: Lucene offers the ability to highlight search terms within the retrieved documents, making it easier for users to identify the context in which the terms appear.\n",
        "\n",
        "Faceted Search: Lucene supports faceted search, which allows users to explore data along multiple dimensions or facets. Facets can be defined on different document attributes, such as categories, tags, or any other metadata."
      ],
      "metadata": {
        "id": "0XpbGOVhfxuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lucene\n",
        "\n",
        "from java.nio.file import Paths\n",
        "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
        "from org.apache.lucene.document import Document, Field, TextField\n",
        "from org.apache.lucene.index import IndexWriter, IndexWriterConfig\n",
        "from org.apache.lucene.search import IndexSearcher\n",
        "from org.apache.lucene.queryparser.classic import QueryParser\n",
        "from org.apache.lucene.store import SimpleFSDirectory\n",
        "from org.apache.lucene.util import Version\n",
        "\n",
        "# Initialize Lucene\n",
        "lucene.initVM()\n",
        "\n",
        "# Specify the path to the index directory\n",
        "index_dir = \"/path/to/index/directory\"\n",
        "\n",
        "# Create the index writer\n",
        "directory = SimpleFSDirectory(Paths.get(index_dir))\n",
        "analyzer = StandardAnalyzer()\n",
        "config = IndexWriterConfig(analyzer)\n",
        "writer = IndexWriter(directory, config)\n",
        "\n",
        "# Create documents and add them to the index\n",
        "doc1 = Document()\n",
        "doc1.add(Field(\"content\", \"This is the first document\", TextField.TYPE_STORED))\n",
        "writer.addDocument(doc1)\n",
        "\n",
        "doc2 = Document()\n",
        "doc2.add(Field(\"content\", \"This is the second document\", TextField.TYPE_STORED))\n",
        "writer.addDocument(doc2)\n",
        "\n",
        "# Commit changes and close the index writer\n",
        "writer.commit()\n",
        "writer.close()\n",
        "\n",
        "# Create the index searcher\n",
        "reader = writer.getReader()\n",
        "searcher = IndexSearcher(reader)\n",
        "\n",
        "# Perform a search\n",
        "query_text = \"first document\"\n",
        "query_parser = QueryParser(\"content\", analyzer)\n",
        "query = query_parser.parse(query_text)\n",
        "hits = searcher.search(query, 10)\n",
        "\n",
        "# Print search results\n",
        "print(\"Search results:\")\n",
        "for hit in hits.scoreDocs:\n",
        "    doc_id = hit.doc\n",
        "    score = hit.score\n",
        "    doc = searcher.doc(doc_id)\n",
        "    content = doc.get(\"content\")\n",
        "    print(f\"Document ID: {doc_id}, Score: {score}, Content: {content}\")\n",
        "\n",
        "# Close the index searcher and reader\n",
        "searcher.getIndexReader().close()\n",
        "directory.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "zq0xX4r5gBfJ",
        "outputId": "a00abab0-d436-484b-82b2-f7457b520dc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84a9fa141982>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlucene\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlucene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlucene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lucene'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}