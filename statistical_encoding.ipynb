{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNboUGRDLceN1E5c3uoR+Pw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GanB/language-models-paper/blob/master/statistical_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "jYumwnxhAfik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is a process used in natural language processing and information retrieval to reduce words to their base or root form, known as the \"stem.\" The purpose of stemming is to normalize words so that variations of the same word can be treated as identical during analysis or search. For example, the words \"run,\" \"running,\" and \"ran\" would all be reduced to the stem \"run.\"\n",
        "\n",
        "Stemming algorithms typically remove common suffixes from words to extract the stem. This process involves rules or heuristics that operate on the word's structure to remove endings such as \"-s,\" \"-ed,\" or \"-ing.\" The resulting stem may not always be a valid word or may be a partial word, but it serves as a common representation for related word forms.\n",
        "\n",
        "Stemming can be useful in various natural language processing tasks, such as text mining, information retrieval, and search engines. By reducing words to their stems, it becomes easier to group similar words together and perform operations like searching or indexing based on the root form of a word. However, it's important to note that stemming algorithms are not always perfect and may produce incorrect stems or remove parts that change the word's meaning.\n",
        "\n",
        "Popular stemming algorithms:\n",
        "\n",
        "* Porter Stemming Algorithm:  It applies a set of rules to strip common English suffixes from words. It is simple and efficient but may sometimes produce stems that are not actual words.\n",
        "\n",
        "* Snowball Stemming Algorithm:  Snowball is an extension of the Porter algorithm. It supports multiple languages and provides more accurate stemming than the original Porter algorithm. Snowball allows for easier customization and the addition of new languages.\n",
        "\n",
        "* Lancaster Stemming Algorithm: The Lancaster stemming algorithm is an aggressive stemming algorithm that applies a series of rules to remove suffixes from words. It is known for its fast execution speed but can sometimes produce very aggressive stems, leading to more drastic reductions than other algorithms.\n",
        "\n",
        "* Lovins Stemming Algorithm: The Lovins stemming algorithm is based on a set of stemming rules developed by J. C. Lovins. It focuses on reducing words to their root forms by applying various transformations. This algorithm is often used for information retrieval tasks.\n",
        "\n",
        "* Porter2 Stemming Algorithm (also known as the English Stemmer): This is an updated version of the Porter stemming algorithm, designed to improve the accuracy of stemming for English words. It addresses some of the limitations of the original Porter algorithm and is commonly used in search engines and information retrieval systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "4exj1PhhCpjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['studying', 'studied', 'running', 'ran', 'sleeping', 'slept', 'flies',\n",
        "         'dies', 'meeting', 'talking', 'talks', 'talked', 'cherries',\n",
        "         'generously', 'cats', 'better', 'rocks', 'wolves' ]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"PorterStemmer:\")\n",
        "\n",
        "for word, stem in zip(words, stemmed_words):\n",
        "    print(f\"Word: {word} | Stem: {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYABIzOsAn52",
        "outputId": "4f13b04d-1690-45f5-f27f-043d80110f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer:\n",
            "Word: studying | Stem: studi\n",
            "Word: studied | Stem: studi\n",
            "Word: running | Stem: run\n",
            "Word: ran | Stem: ran\n",
            "Word: sleeping | Stem: sleep\n",
            "Word: slept | Stem: slept\n",
            "Word: flies | Stem: fli\n",
            "Word: dies | Stem: die\n",
            "Word: meeting | Stem: meet\n",
            "Word: talking | Stem: talk\n",
            "Word: talks | Stem: talk\n",
            "Word: talked | Stem: talk\n",
            "Word: cherries | Stem: cherri\n",
            "Word: generously | Stem: gener\n",
            "Word: cats | Stem: cat\n",
            "Word: better | Stem: better\n",
            "Word: rocks | Stem: rock\n",
            "Word: wolves | Stem: wolv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "words = ['studying', 'studied', 'running', 'ran', 'sleeping', 'slept', 'flies',\n",
        "         'dies', 'meeting', 'talking', 'talks', 'talked', 'cherries',\n",
        "         'generously', 'cats', 'better', 'rocks', 'wolves' ]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"SnowballStemmer:\")\n",
        "for word, stem in zip(words, stemmed_words):\n",
        "    print(f\"Word: {word} | Stem: {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb00CMVcGFNY",
        "outputId": "9768b771-c14b-4318-f1dd-f51e3a8570c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SnowballStemmer:\n",
            "Word: studying | Stem: studi\n",
            "Word: studied | Stem: studi\n",
            "Word: running | Stem: run\n",
            "Word: ran | Stem: ran\n",
            "Word: sleeping | Stem: sleep\n",
            "Word: slept | Stem: slept\n",
            "Word: flies | Stem: fli\n",
            "Word: dies | Stem: die\n",
            "Word: meeting | Stem: meet\n",
            "Word: talking | Stem: talk\n",
            "Word: talks | Stem: talk\n",
            "Word: talked | Stem: talk\n",
            "Word: cherries | Stem: cherri\n",
            "Word: generously | Stem: generous\n",
            "Word: cats | Stem: cat\n",
            "Word: better | Stem: better\n",
            "Word: rocks | Stem: rock\n",
            "Word: wolves | Stem: wolv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "Q0M6jTkQE0MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a natural language processing technique that aims to determine the base or dictionary form of a word, called the \"lemma.\" Unlike stemming, which truncates words to their root form, lemmatization considers the word's context and grammatical role to derive the canonical form.\n",
        "\n",
        "The process of lemmatization involves analyzing words based on their part of speech (POS) tags, such as noun, verb, adjective, adverb, etc., and applying morphological rules to transform them to their base form. This base form is typically a valid word that can be found in a dictionary.\n",
        "\n",
        "For example, the lemma of the word \"running\" would be \"run,\" and the lemma of \"better\" would be \"good.\" Lemmatization takes into account factors like tense, plurality, and inflection to ensure accurate normalization.\n",
        "\n",
        "Lemmatization offers more accurate results compared to stemming because it considers the context and semantics of words. It can be useful in various natural language processing applications, such as text analysis, information retrieval, machine translation, and sentiment analysis.\n",
        "\n",
        "However, lemmatization can be computationally more expensive than stemming due to the need for dictionary lookups and morphological analysis. It also requires the availability of linguistic resources like POS taggers and lemmatization rules specific to the language being processed.\n",
        "\n",
        "Popular Lemmatization algorithms:\n",
        "\n",
        "* WordNet Lemmatizer: WordNet is a lexical database that includes information about word senses and relationships. The WordNet lemmatizer maps words to their corresponding lemmas based on the WordNet database. It is commonly used in applications that require English lemmatization.\n",
        "\n",
        "* Stanford CoreNLP Lemmatizer: Stanford CoreNLP is a popular natural language processing toolkit that provides lemmatization functionality. Its lemmatizer utilizes a combination of rule-based approaches and machine learning techniques to determine the lemma of a word. It supports multiple languages and can handle various word forms and inflections.\n",
        "\n",
        "* spaCy Lemmatizer: spaCy is a widely used library for natural language processing in Python. It includes a lemmatizer component that applies lemmatization based on the word's POS tag and syntactic dependencies. spaCy supports multiple languages and provides accurate lemmatization results.\n",
        "\n",
        "* TreeTagger: TreeTagger is a part-of-speech tagger and lemmatizer developed by the Natural Language Processing Group at the University of Stuttgart. It utilizes a combination of rule-based and stochastic methods to perform lemmatization for several languages. TreeTagger is known for its accuracy and wide language coverage.\n",
        "\n",
        "* Morpha: Morpha is a lemmatization tool that applies morphological analysis to derive lemmas. It uses finite-state transducers to handle different word forms and inflections. Morpha is particularly useful for English lemmatization and is known for its speed and accuracy.\n"
      ],
      "metadata": {
        "id": "5ziPIYsLE3au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeQxVzd3LFr3",
        "outputId": "f7941442-6745-419e-8a43-52dac05c0685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['studying', 'studied', 'running', 'ran', 'sleeping', 'slept', 'flies',\n",
        "         'dies', 'meeting', 'talking', 'talks', 'talked', 'cherries',\n",
        "         'generously', 'cats', 'better', 'rocks', 'wolves' ]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "\n",
        "print('WordNetLemmatizer:')\n",
        "for word, lemma in zip(words, lemmatized_words):\n",
        "    print(f\"Word: {word} | Lemma: {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSO384J-IrTe",
        "outputId": "568d1a07-4cc4-486d-ef01-bfe2abe2bfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNetLemmatizer:\n",
            "Word: studying | Lemma: studying\n",
            "Word: studied | Lemma: studied\n",
            "Word: running | Lemma: running\n",
            "Word: ran | Lemma: ran\n",
            "Word: sleeping | Lemma: sleeping\n",
            "Word: slept | Lemma: slept\n",
            "Word: flies | Lemma: fly\n",
            "Word: dies | Lemma: dy\n",
            "Word: meeting | Lemma: meeting\n",
            "Word: talking | Lemma: talking\n",
            "Word: talks | Lemma: talk\n",
            "Word: talked | Lemma: talked\n",
            "Word: cherries | Lemma: cherry\n",
            "Word: generously | Lemma: generously\n",
            "Word: cats | Lemma: cat\n",
            "Word: better | Lemma: better\n",
            "Word: rocks | Lemma: rock\n",
            "Word: wolves | Lemma: wolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "lemmatized_words = []\n",
        "for word, pos in pos_tags:\n",
        "    if pos.startswith('N'):  # Noun\n",
        "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
        "    elif pos.startswith('V'):  # Verb\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    elif pos.startswith('J'):  # Adjective\n",
        "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
        "    elif pos.startswith('R'):  # Adverb\n",
        "        lemma = lemmatizer.lemmatize(word, pos='r')\n",
        "    else:\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "    lemmatized_words.append(lemma)\n",
        "\n",
        "for token, pos, lemma in zip(tokens, pos_tags, lemmatized_words):\n",
        "    print(f\"Token: {token} | POS: {pos[1]} | Lemma: {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUnaQpviMdCJ",
        "outputId": "c7c30972-a267-49c1-cbbe-276d387e42f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The | POS: DT | Lemma: The\n",
            "Token: quick | POS: JJ | Lemma: quick\n",
            "Token: brown | POS: NN | Lemma: brown\n",
            "Token: fox | POS: NN | Lemma: fox\n",
            "Token: jumps | POS: VBZ | Lemma: jump\n",
            "Token: over | POS: IN | Lemma: over\n",
            "Token: the | POS: DT | Lemma: the\n",
            "Token: lazy | POS: JJ | Lemma: lazy\n",
            "Token: dog | POS: NN | Lemma: dog\n",
            "Token: . | POS: . | Lemma: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "P0t6CvtaTcKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer is a feature extraction technique commonly used in natural language processing (NLP) to convert a collection of text documents into a matrix of token counts.\n",
        "\n",
        "The CountVectorizer performs the following steps:\n",
        "\n",
        "* Tokenization: It breaks down the text into individual words or terms called tokens. It removes punctuation and converts the text to lowercase by default. It also allows customization of tokenization rules.\n",
        "\n",
        "* Vocabulary Building: It constructs a vocabulary of unique tokens from the text data. Each unique token becomes a feature in the resulting matrix. The vocabulary is typically represented as a dictionary where the keys are the tokens, and the values are the indices or positions of the tokens in the matrix.\n",
        "\n",
        "* Counting: It counts the occurrence of each token in each document. The resulting matrix, called the document-term matrix, represents the frequency of tokens in the text data. Each row corresponds to a document, and each column corresponds to a token in the vocabulary. The matrix contains the count of each token in each document.\n",
        "\n",
        "The resulting document-term matrix can be used as input to various machine learning algorithms, such as classification or clustering algorithms. It represents the text data in a numerical form that algorithms can understand and process."
      ],
      "metadata": {
        "id": "ntmrKWXAUnIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"I love cats\",\n",
        "    \"I love dogs\",\n",
        "    \"Cats and dogs are pets\",\n",
        "    \"Dogs are loyal\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"Document-Term Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "IJZR80-1Tgl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09eaf588-e6f0-4ecf-82ab-5dacdf4c4c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'are' 'cats' 'dogs' 'love' 'loyal' 'pets']\n",
            "Document-Term Matrix:\n",
            "[[0 0 1 0 1 0 0]\n",
            " [0 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 1]\n",
            " [0 1 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF\n"
      ],
      "metadata": {
        "id": "AkRm6QqqANtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used in natural language processing and information retrieval to determine the importance of a term (word) in a document relative to a collection of documents.\n",
        "\n",
        "TF-IDF combines two components: term frequency (TF) and inverse document frequency (IDF).\n",
        "\n",
        "* Term Frequency (TF): It measures the frequency of a term (word) within a document. It is calculated by dividing the number of occurrences of a term in a document by the total number of terms in that document. The intuition behind TF is that the more frequent a term is in a document, the more likely it is to be important for that document.\n",
        "\n",
        "* Inverse Document Frequency (IDF): It measures the rarity or uniqueness of a term across all documents in a collection. IDF is calculated by taking the logarithm of the ratio between the total number of documents and the number of documents containing the term. The intuition behind IDF is that terms that occur in a smaller number of documents are more informative and carry more weight compared to terms that occur in a larger number of documents.\n",
        "\n",
        "The TF-IDF score of a term in a document is obtained by multiplying its term frequency (TF) in the document with the inverse document frequency (IDF) of the term.\n",
        "\n",
        "The TF-IDF representation of a document collection can be used for various tasks, such as document retrieval, text classification, and information extraction. It allows us to represent documents in a numerical form that captures the importance of terms within the documents and across the collection."
      ],
      "metadata": {
        "id": "hgzEf1TRQhKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example text documents\n",
        "documents = [\n",
        "    \"I love cats\",\n",
        "    \"I love dogs\",\n",
        "    \"Cats and dogs are pets\",\n",
        "    \"Dogs are loyal\",\n",
        "]\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Learn the vocabulary and transform the documents into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary (tokens) and the TF-IDF matrix\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ygElM3TciTM",
        "outputId": "fa8919dc-61b8-4843-b13b-33e8b92e971b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'are' 'cats' 'dogs' 'love' 'loyal' 'pets']\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.70710678 0.         0.70710678 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.62922751 0.77722116 0.\n",
            "  0.        ]\n",
            " [0.52338122 0.41263976 0.41263976 0.33406745 0.         0.\n",
            "  0.52338122]\n",
            " [0.         0.55349232 0.         0.44809973 0.         0.70203482\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Hot Encoding"
      ],
      "metadata": {
        "id": "YdiwSN0YTUxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a process used to represent categorical variables as binary vectors. It is a technique commonly employed in machine learning and data preprocessing tasks.\n",
        "\n",
        "In one-hot encoding, each category or value in a categorical variable is transformed into a binary vector representation. This binary vector has the length equal to the number of unique categories in the variable. It contains all zeros except for a single one at the index corresponding to the category.\n",
        "\n",
        "Here's an example to illustrate one-hot encoding:\n",
        "\n",
        "Suppose we have a categorical variable \"Color\" with three possible categories: \"Red\", \"Green\", and \"Blue\". One-hot encoding would transform this variable into three binary features: \"Color_Red\", \"Color_Green\", and \"Color_Blue\".\n",
        "\n",
        "Color\tColor_Red\tColor_Green\tColor_Blue\n",
        "Red\t1\t0\t0\n",
        "Green\t0\t1\t0\n",
        "Blue\t0\t0\t1\n",
        "In this example, each row represents an instance or observation with a specific color. The one-hot encoded representation indicates the presence of a particular color by setting the corresponding binary feature to 1 and the others to 0.\n",
        "\n",
        "One-hot encoding is useful for machine learning algorithms as it allows categorical variables to be represented in a format that can be easily understood and processed by models. It enables the inclusion of categorical information in numerical computations and avoids any ordinal relationship assumption between categories."
      ],
      "metadata": {
        "id": "II0tjdNuHk33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzPOEVm-FVtF",
        "outputId": "8e2e097e-103d-4552-bb55-8f9b1296d2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categories = ['Red', 'Green', 'Blue']\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "encoded_data = encoder.fit_transform([[category] for category in categories]).toarray()\n",
        "\n",
        "print(encoded_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"I love cats\",\n",
        "    \"I love dogs\",\n",
        "    \"Cats and dogs are pets\",\n",
        "    \"Dogs are loyal\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"One-Hot Encoded Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3H-KT2AKBR_",
        "outputId": "a6cc6dde-0d88-4265-dd8b-82dd68ea3d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['and' 'are' 'cats' 'dogs' 'love' 'loyal' 'pets']\n",
            "One-Hot Encoded Matrix:\n",
            "[[0 0 1 0 1 0 0]\n",
            " [0 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 1]\n",
            " [0 1 0 1 0 1 0]]\n"
          ]
        }
      ]
    }
  ]
}